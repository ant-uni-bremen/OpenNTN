{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final framework we will have a look at the integratebility of ML and and the NTN channel models.\n",
    "\n",
    "The main feature of this framework is the fast and easy integration of ML based solutions. This includes both the \n",
    "1. integration of existing solutions,\n",
    "2. as well as the setup of new solutions based on for example architectures from literature. \n",
    "In this notebook we will showcase the process by implementing a Rx processing chain for OFDM systems. We assume the following scenario:\n",
    "\n",
    "You are a reasearcher and are interested in the investigation of ML based solutions for NTN. During your research, you discover that there already is freely available code for a NN based solution for the SIMO Rx processing of terrestrial networks. You quickly reuse the code and investigate it in your NTN use case. After observing the performance, you try to adpat the code for the MIMO case. While doing more research, you find that there is already a paper on adapting the NN for the MIMO scenarios. However, this time there is no code. So you quickly, implement the code yourself and test it in the flexible framework.\n",
    "\n",
    "The used papers are the paper on a NN based SIMO receiver\n",
    "DeepRx: Fully convolutional deep learning receiver, submitted to IEEE Transactions on Wireless Communications. arXiv preprint:1711.05101, 2020.\n",
    "Which has already been implemented in a Sionna Tutorial:\n",
    "Neural Receiver for OFDM SIMO Systems, https://nvlabs.github.io/sionna/examples/Neural_Receiver.html\n",
    "The extension for the MIMO case is in the paper:\n",
    "Korpi, D., Honkala, M., Huttunen, J.M., & Starck, V. (2020). DeepRx MIMO: Convolutional MIMO Detection with Learned Multiplicative Transformations. ICC 2021 - IEEE International Conference on Communications, 1-7.\n",
    "\n",
    "As the previous examples have shown, the NTN channels are very challenging, especially without perfect CSI and in presense of doppler spread. Thus, ML based solutions are a promissing solution. Additionally, despite this notebook reusing an existing NN already implemented in Sionna for the existing code, this actually only effects a small part of the code and could be done with any existing NN in a Tensorflow compatible format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import sionna\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#Setup to use GPU\n",
    "import os\n",
    "gpu_num = 0 # Use \"\" to use the CPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "\n",
    "#Specific components used in this simulation\n",
    "from sionna.ofdm import ResourceGrid, ResourceGridMapper, LSChannelEstimator, LMMSEEqualizer, RemoveNulledSubcarriers, ResourceGridDemapper\n",
    "from sionna.mimo import StreamManagement\n",
    "from sionna.channel.tr38811 import AntennaArray, DenseUrban, Urban, SubUrban\n",
    "from sionna.utils import BinarySource, ebnodb2no, sim_ber, log10, insert_dims, expand_to_rank\n",
    "from sionna.fec.ldpc.encoding import LDPC5GEncoder\n",
    "from sionna.fec.ldpc.decoding import LDPC5GDecoder\n",
    "from sionna.mapping import Mapper, Demapper\n",
    "from sionna.channel import OFDMChannel\n",
    "\n",
    "#Used to track time as save results\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sionna.channel.tr38811.utils import gen_single_sector_topology as gen_ntn_topology\n",
    "\n",
    "#Imports to build the Neural Networks\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "sionna.config.xla_compat=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System steup parameters\n",
    "num_conv_channels = 128\n",
    "num_bits_per_symbol = 2\n",
    "train_new_model = False\n",
    "evaluate_model = True\n",
    "model_weights_path = \"ntn_deeprx_model_weights_no_Dopp\"\n",
    "\n",
    "ebno_db_min = -5.0\n",
    "ebno_db_max = 16.0\n",
    "ebno_dbs = np.arange(ebno_db_min, # Min SNR for evaluation\n",
    "                    ebno_db_max, # Max SNR for evaluation\n",
    "                    0.5) # Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure uses a repeated architecture using so called residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(Layer):\n",
    "    r\"\"\"\n",
    "    This Keras layer implements a convolutional residual block made of two convolutional layers with ReLU activation, layer normalization, and a skip connection.\n",
    "    The number of convolutional channels of the input must match the number of kernel of the convolutional layers ``num_conv_channel`` for the skip connection to work.\n",
    "\n",
    "    Input\n",
    "    ------\n",
    "    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float\n",
    "        Input of the layer\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    : [batch size, num time samples, num subcarriers, num_conv_channel], tf.float\n",
    "        Output of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
    "        self._layer_norm_1 = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self._conv_1 = Conv2D(filters=num_conv_channels,\n",
    "                              kernel_size=[3,3],\n",
    "                              padding='same',\n",
    "                              activation=None)\n",
    "        # Layer normalization is done over the last three dimensions: time, frequency, conv 'channels'\n",
    "        self._layer_norm_2 = LayerNormalization(axis=(-1, -2, -3))\n",
    "        self._conv_2 = Conv2D(filters=num_conv_channels,\n",
    "                              kernel_size=[3,3],\n",
    "                              padding='same',\n",
    "                              activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = self._layer_norm_1(inputs)\n",
    "        z = relu(z)\n",
    "        z = self._conv_1(z)\n",
    "        z = self._layer_norm_2(z)\n",
    "        z = relu(z)\n",
    "        z = self._conv_2(z)\n",
    "        # Skip connection\n",
    "        z = z + inputs\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural receiver which takes the received IQ symbols and calculates the corresponding LLRs for the demapper is implemented in the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralReceiver(Layer):\n",
    "    r\"\"\"\n",
    "    Keras layer implementing a residual convolutional neural receiver.\n",
    "\n",
    "    This neural receiver is fed with the post-DFT received samples, forming a resource grid of size num_of_symbols x fft_size, and computes LLRs on the transmitted coded bits.\n",
    "    These LLRs can then be fed to an outer decoder to reconstruct the information bits.\n",
    "\n",
    "    As the neural receiver is fed with the entire resource grid, including the guard bands and pilots, it also computes LLRs for these resource elements.\n",
    "    They must be discarded to only keep the LLRs corresponding to the data-carrying resource elements.\n",
    "\n",
    "    Input\n",
    "    ------\n",
    "    y : [batch size, num rx antenna, num ofdm symbols, num subcarriers], tf.complex\n",
    "        Received post-DFT samples.\n",
    "\n",
    "    no : [batch size], tf.float32\n",
    "        Noise variance. At training, a different noise variance value is sampled for each batch example.\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    : [batch size, num ofdm symbols, num subcarriers, num_bits_per_symbol]\n",
    "        LLRs on the transmitted bits.\n",
    "        LLRs computed for resource elements not carrying data (pilots, guard bands...) must be discarded.\n",
    "    \"\"\"\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # Input convolution\n",
    "        self._input_conv = Conv2D(filters=num_conv_channels,\n",
    "                                  kernel_size=[3,3],\n",
    "                                  padding='same',\n",
    "                                  activation=None)\n",
    "        # Residual blocks\n",
    "        self._res_block_1 = ResidualBlock()\n",
    "        self._res_block_2 = ResidualBlock()\n",
    "        self._res_block_3 = ResidualBlock()\n",
    "        self._res_block_4 = ResidualBlock()\n",
    "        # Output conv\n",
    "        self._output_conv = Conv2D(filters=num_bits_per_symbol,\n",
    "                                   kernel_size=[3,3],\n",
    "                                   padding='same',\n",
    "                                   activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y, no = inputs\n",
    "\n",
    "        # Feeding the noise power in log10 scale helps with the performance\n",
    "        no = log10(no)\n",
    "\n",
    "        # Stacking the real and imaginary components of the different antennas along the 'channel' dimension\n",
    "        y = tf.transpose(y, [0, 2, 3, 1]) # Putting antenna dimension last\n",
    "        no = insert_dims(no, 3, 1)\n",
    "        no = tf.tile(no, [1, y.shape[1], y.shape[2], 1])\n",
    "        # z : [batch size, num ofdm symbols, num subcarriers, 2*num rx antenna + 1]\n",
    "        z = tf.concat([tf.math.real(y),\n",
    "                       tf.math.imag(y),\n",
    "                       no], axis=-1)\n",
    "        # Input conv\n",
    "        z = self._input_conv(z)\n",
    "        # Residual blocks\n",
    "        z = self._res_block_1(z)\n",
    "        z = self._res_block_2(z)\n",
    "        z = self._res_block_3(z)\n",
    "        z = self._res_block_4(z)\n",
    "        # Output conv\n",
    "        z = self._output_conv(z)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the end to end model in the following cell. To save resources while training, the decoding is only exectued during the inference, during training the system is optimized by comparing the LLRs. If deep_rx is False, the model instead builds a traditional communications system with or without perfect csi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullE2EModel(tf.keras.Model):\n",
    "    \"\"\"Simulate OFDM MIMO transmissions over a 3GPP 38.811 model.\n",
    "    \"\"\"\n",
    "    def __init__(self, scenario, perfect_csi, doppler_enabled, elevation_angle = 80.0, deep_rx = False, training=False):\n",
    "        super().__init__()\n",
    "        self._scenario = scenario\n",
    "        self._perfect_csi = perfect_csi\n",
    "        self._training = training\n",
    "\n",
    "        # Internally set parameters\n",
    "        self._carrier_frequency = 30.0e9\n",
    "        self._fft_size = 128\n",
    "        self._subcarrier_spacing = 30e3\n",
    "        self._num_ofdm_symbols = 14\n",
    "        self._cyclic_prefix_length = 20\n",
    "        self._pilot_ofdm_symbol_indices = [2, 11]\n",
    "        self._num_bs_ant = 8\n",
    "        self._num_ut = 1\n",
    "        self._num_ut_ant = 1\n",
    "        self._num_bits_per_symbol = 2\n",
    "        self._coderate = 0.5\n",
    "        self._direction = \"uplink\"\n",
    "        self._sat_height = 600000.0\n",
    "        self._deep_rx = deep_rx\n",
    "\n",
    "        # Create an RX-TX association matrix\n",
    "        # rx_tx_association[i,j]=1 means that receiver i gets at least one stream\n",
    "        # from transmitter j. Depending on the transmission direction (uplink or downlink),\n",
    "        # the role of UT and BS can change.\n",
    "        bs_ut_association = np.zeros([1, self._num_ut])\n",
    "        bs_ut_association[0, :] = 1\n",
    "        self._rx_tx_association = bs_ut_association\n",
    "        self._num_tx = self._num_ut\n",
    "        self._num_streams_per_tx = self._num_ut_ant\n",
    "\n",
    "        # Setup an OFDM Resource Grid\n",
    "        self._rg = ResourceGrid(num_ofdm_symbols=self._num_ofdm_symbols,\n",
    "                                fft_size=self._fft_size,\n",
    "                                subcarrier_spacing=self._subcarrier_spacing,\n",
    "                                num_tx=self._num_tx,\n",
    "                                num_streams_per_tx=self._num_streams_per_tx,\n",
    "                                cyclic_prefix_length=self._cyclic_prefix_length,\n",
    "                                pilot_pattern=\"kronecker\",\n",
    "                                pilot_ofdm_symbol_indices=self._pilot_ofdm_symbol_indices)\n",
    "\n",
    "        # Setup StreamManagement\n",
    "        self._sm = StreamManagement(self._rx_tx_association, self._num_streams_per_tx)\n",
    "\n",
    "        # Configure antenna arrays\n",
    "        self._ut_array = AntennaArray(\n",
    "                                num_rows=1,\n",
    "                                num_cols=1,\n",
    "                                polarization=\"single\",\n",
    "                                polarization_type=\"V\",\n",
    "                                antenna_pattern=\"omni\",\n",
    "                                carrier_frequency=self._carrier_frequency)\n",
    "\n",
    "        self._bs_array = AntennaArray(num_rows=1,\n",
    "                                    num_cols=int(self._num_bs_ant/2),\n",
    "                                    polarization=\"dual\",\n",
    "                                    polarization_type=\"cross\",\n",
    "                                    antenna_pattern=\"38.901\",\n",
    "                                    carrier_frequency=self._carrier_frequency)\n",
    "\n",
    "        # Configure the channel model\n",
    "        if scenario == \"dur\":\n",
    "            self._channel_model = DenseUrban(carrier_frequency=self._carrier_frequency,\n",
    "                                    ut_array=self._ut_array,\n",
    "                                    bs_array=self._bs_array,\n",
    "                                    direction=self._direction,\n",
    "                                    elevation_angle=elevation_angle,\n",
    "                                    doppler_enabled=doppler_enabled)\n",
    "        elif scenario == \"sur\":\n",
    "            self._channel_model = SubUrban(carrier_frequency=self._carrier_frequency,\n",
    "                                    ut_array=self._ut_array,\n",
    "                                    bs_array=self._bs_array,\n",
    "                                    direction=self._direction,\n",
    "                                    elevation_angle=elevation_angle,\n",
    "                                    doppler_enabled=doppler_enabled)\n",
    "        elif scenario == \"urb\":\n",
    "            self._channel_model = Urban(carrier_frequency=self._carrier_frequency,\n",
    "                                    ut_array=self._ut_array,\n",
    "                                    bs_array=self._bs_array,\n",
    "                                    direction=self._direction,\n",
    "                                    elevation_angle=elevation_angle,\n",
    "                                    doppler_enabled=doppler_enabled)\n",
    "\n",
    "        # Instantiate other building blocks\n",
    "        self._binary_source = BinarySource()\n",
    "\n",
    "\n",
    "        self._n = int(self._rg.num_data_symbols*self._num_bits_per_symbol) # Number of coded bits\n",
    "        self._k = int(self._n*self._coderate)                              # Number of information bits\n",
    "        # To reduce the computational complexity of training, the outer code is not used when training,\n",
    "        # as it is not required\n",
    "        if not training:\n",
    "            self._encoder = LDPC5GEncoder(self._k, self._n)\n",
    "        self._mapper = Mapper(\"qam\", self._num_bits_per_symbol)\n",
    "        self._rg_mapper = ResourceGridMapper(self._rg)\n",
    "\n",
    "        self._ofdm_channel = OFDMChannel(self._channel_model, self._rg, add_awgn=True,\n",
    "                                        normalize_channel=True, return_channel=True)\n",
    "\n",
    "        ## Receiver either with perf csi, without perf csi and deeprx\n",
    "        if self._perfect_csi:\n",
    "            self._remove_nulled_subcarriers = RemoveNulledSubcarriers(self._rg)\n",
    "        else:\n",
    "            self._ls_est = LSChannelEstimator(self._rg, interpolation_type=\"nn\")\n",
    "\n",
    "        self._lmmse_equ = LMMSEEqualizer(self._rg, self._sm)\n",
    "        self._demapper = Demapper(\"app\", \"qam\", self._num_bits_per_symbol)\n",
    "\n",
    "        if self._deep_rx:\n",
    "            self._neural_receiver = NeuralReceiver()\n",
    "            self._rg_demapper = ResourceGridDemapper(self._rg, self._sm)\n",
    "\n",
    "        if not self._training:\n",
    "            self._decoder = LDPC5GDecoder(self._encoder, hard_out=True)\n",
    "\n",
    "    def new_topology(self, batch_size):\n",
    "        topology = gen_ntn_topology(batch_size=batch_size, num_ut=self._num_ut, scenario=self._scenario)\n",
    "        self._channel_model.set_topology(*topology)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, batch_size, ebno_db):\n",
    "        self.new_topology(batch_size)\n",
    "        # If `ebno_db` is a scalar, a tensor with shape [batch size] is created as it is what is expected by some layers\n",
    "        if len(ebno_db.shape) == 0:\n",
    "            ebno_db = tf.fill([batch_size], ebno_db)\n",
    "        no = ebnodb2no(ebno_db, self._num_bits_per_symbol, self._coderate)\n",
    "\n",
    "        if self._training:\n",
    "            c = self._binary_source([batch_size, self._num_tx, self._num_streams_per_tx, self._n])\n",
    "        else:\n",
    "            b = self._binary_source([batch_size, self._num_tx, self._num_streams_per_tx, self._k])\n",
    "            c = self._encoder(b)\n",
    "\n",
    "        x = self._mapper(c)\n",
    "        x_rg = self._rg_mapper(x)\n",
    "\n",
    "        no_ = expand_to_rank(no, tf.rank(x_rg))\n",
    "        y, h = self._ofdm_channel([x_rg, no_])\n",
    "\n",
    "        if not self._deep_rx:\n",
    "            if self._perfect_csi:\n",
    "                h_hat = self._remove_nulled_subcarriers(h)\n",
    "                err_var = 0.0\n",
    "            else:\n",
    "                h_hat, err_var = self._ls_est([y, no])\n",
    "            \n",
    "            x_hat, no_eff = self._lmmse_equ([y, h_hat, err_var, no])\n",
    "            llr = self._demapper([x_hat, no_eff])   \n",
    "\n",
    "        elif self._deep_rx:\n",
    "            y = tf.squeeze(y, axis=1)\n",
    "            llr = self._neural_receiver([y, no])\n",
    "            llr = insert_dims(llr, 2, 1) # Reshape the input to fit what the resource grid demapper is expected\n",
    "            llr = tf.reshape(llr, [batch_size, 1, 1, self._rg.num_ofdm_symbols, self._rg.fft_size, 2])\n",
    "\n",
    "            llr = self._rg_demapper(llr) # Extract data-carrying resource elements. The other LLrs are discarded\n",
    "            llr = tf.reshape(llr, [batch_size, self._num_tx, self._num_streams_per_tx, self._n]) # Reshape the LLRs to fit what the outer decoder is expected\n",
    "\n",
    "\n",
    "        # Outer coding is not needed if the information rate is returned\n",
    "        if self._training:\n",
    "            bce = tf.nn.sigmoid_cross_entropy_with_logits(c, llr)\n",
    "            bce = tf.reduce_mean(bce)\n",
    "            rate = tf.constant(1.0, tf.float32) - bce/tf.math.log(2.)\n",
    "            return rate\n",
    "        else:\n",
    "            b_hat = self._decoder(llr)\n",
    "            return b,b_hat # Ground truth and reconstructed information bits returned for BER/BLER computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model and save the progress every 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FullE2EModel(scenario=\"dur\", perfect_csi=False, doppler_enabled=False, elevation_angle = 80.0, deep_rx = True, training=True)\n",
    "training_batch_size = 128\n",
    "num_training_iterations = 30000\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for i in range(num_training_iterations):\n",
    "    # Sampling a batch of SNRs\n",
    "    ebno_db = tf.random.uniform(shape=[], minval=ebno_db_min, maxval=ebno_db_max)\n",
    "    # Forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        rate = model(training_batch_size, ebno_db)\n",
    "        # Tensorflow optimizers only know how to minimize loss function.\n",
    "        # Therefore, a loss function is defined as the additive inverse of the BMD rate\n",
    "        loss = -rate\n",
    "    # Computing and applying gradients\n",
    "    weights = model.trainable_weights\n",
    "    grads = tape.gradient(loss, weights)\n",
    "    optimizer.apply_gradients(zip(grads, weights))\n",
    "    # Periodically printing the progress\n",
    "    if i % 100 == 0:\n",
    "        print('Iteration {}/{}  Rate: {:.4f} bit'.format(i, num_training_iterations, rate.numpy()), end='\\r')\n",
    "        with open(model_weights_path, 'wb') as f:\n",
    "            pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model by executing inference with the trained weights. We save the results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "with open(model_weights_path, 'wb') as f:\n",
    "    pickle.dump(weights, f)\n",
    "    \n",
    "start = time.time()\n",
    "SIMS = {\n",
    "    \"ebno_db\" : list(np.arange(-5, 16, 0.5)),\n",
    "    \"ber\" : [],\n",
    "    \"bler\" : [],\n",
    "    \"duration\" : None\n",
    "}\n",
    "\n",
    "model = model = FullE2EModel(scenario=\"dur\", perfect_csi=False, doppler_enabled=False, elevation_angle = 80.0, deep_rx = True, training=False)\n",
    "\n",
    "# Run one inference to build the layers and loading the weights\n",
    "model(1, tf.constant(10.0, tf.float32))\n",
    "with open(model_weights_path, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "model.set_weights(weights)\n",
    "BLER = {}\n",
    "# Evaluations\n",
    "ber,bler = sim_ber(model, ebno_dbs, batch_size=128, num_target_block_errors=1000, max_mc_iter=1000)\n",
    "SIMS[\"ber\"].append(list(ber.numpy()))\n",
    "SIMS[\"bler\"].append(list(bler.numpy()))\n",
    "\n",
    "SIMS[\"duration\"] = time.time() -  start\n",
    "print(\"Simulations took \", SIMS[\"duration\"])\n",
    "\n",
    "with open('ntn_deeprx_sim_results.pkl', 'wb') as f:\n",
    "    pickle.dump(SIMS, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the simulations might take a while, the following cell saves the results directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = [0.00017755126953125, 0.00011601765950520834, 5.492655436197917e-05, 2.7776082356770834e-05, 1.6774495442708332e-05, 7.802327473958333e-06, 3.6824544270833335e-06, 5.223592122395833e-06, 4.786173502604166e-06, 1.3682047526041668e-06, 4.933675130208333e-07]\n",
    "bler = [0.0014140625, 0.0008125, 0.00046875, 0.0002421875, 0.000109375, 7.03125e-05, 3.125e-05, 4.6875e-05, 2.34375e-05, 7.8125e-06, 7.8125e-06]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sionna_kernel",
   "language": "python",
   "name": "sionna_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
